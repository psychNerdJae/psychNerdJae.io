---
title: "Title here"
date: February 21, 2021
output:
  distill::distill_article:
    toc: true
    toc_float: true
    self_contained: false
categories:
  - psych
  - grad school
---

This blog post was originally written on DATE. It was last updated on `r Sys.Date()` using R Markdown.

# Optimization

# Linear regression

```{r}
library(tidyverse)
library(broom)
library(palmerpenguins)

penguins %>%
  drop_na() %>%
  ggplot(aes(x=sex, y=body_mass_g)) +
  facet_wrap(~species) +
  geom_point(alpha = 0.2) +
  stat_summary(aes(group = 1),
               geom = "line", fun = mean,
               color = "red", size = 1)

penguins %>%
  drop_na() %>%
  lm(body_mass_g ~ sex + species,
     data = .) %>%
  tidy()
```

```{r}
penguins_data <- penguins %>%
  drop_na() %>%
  mutate(is_male = (sex == "male"),
         is_chinstrap = (species == "Chinstrap"),
         is_gentoo = (species == "Gentoo")) %>%
  mutate(across(starts_with("is_"), .fns = as.numeric)) %>%
  select(body_mass_g, starts_with("is_"))

penguins_data %>%
  slice_head(n = 10)

obj_fun_pegnuins <- function(these_params, this_data) {
  # Unpack parameters
  beta_intercept <- these_params[1]
  beta_male <- these_params[2]
  beta_chinstrap <- these_params[3]
  beta_gentoo <- these_params[4]
  
  # Unpack data
  body_mass <- this_data$body_mass_g
  is_male <- this_data$is_male
  is_chinstrap <- this_data$is_chinstrap
  is_gentoo <- this_data$is_gentoo
  
  linear_combination <- (beta_intercept + 
                           beta_male * is_male +
                           beta_chinstrap * is_chinstrap +
                           beta_gentoo * is_gentoo)
  
  # Prediction error: difference between what was observed vs what was predicted
  residual_error <- body_mass - linear_combination
  sum_squared_error <- sum(residual_error^2)
  
  # Our objective (hence, objective function) is to choose beta weights that
  # minimize the sum of squared error... Now that we have a function that tells
  # us how much error is generated (given some set of beta weights), we can run
  # this function through the optimizer many times until we find the weights
  # that do the best job of accomplishing our objective
  return (sum_squared_error)
}

test <- optim(
  # Make initial guesses about the parameter values
  par = c(0, 0, 0, 0),
  # Tell the optimizer what our objective function is
  fn = obj_fun_pegnuins,
  # Usually, ... contains the data argument(s) for the objective function
  this_data = penguins_data,
  # The optimizer will keep guessing until it settles on an answer,
  # or else gives up after `maxit` iterations.
  control = list(maxit = 5000)
)

test
```


```{r}
optim_to_tibble <- function(optim_output, param_names) {
  tibble(
    parameter = param_names,
    value = optim_output$par
  ) %>%
    mutate(
      neg_loglik = optim_output$value,
      convergence = case_when(
        optim_output$convergence == 0 ~ "converged",
        optim_output$convergence == 1 ~ "maxit reached",
        optim_output$convergence == 10 ~ "simplex degeneracy",
        TRUE ~ "unknown problem"
      )
    )
}

run_optim <- function(how_many_runs, max_iter_per_run,
                      objective_function, param_names, param_guesses, ...) {
  this_run <- optim(
    par = param_guesses,
    fn = objective_function,
    # Usually, ... contains the data argument(s) for the objective function
    ... = ...,
    control = list(maxit = max_iter_per_run)
  ) %>%
    optim_to_tibble(param_names)
}
```



